import streamlit as st
import pandas as pd
from supabase import create_client
from openai import OpenAI
import os
from dotenv import load_dotenv
from datetime import datetime
import time
import json
import re
import requests
from bs4 import BeautifulSoup
import numpy as np

# LangGraph ê´€ë ¨
from typing import TypedDict, Annotated, List, Union, Dict
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
import operator

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ (LangGraph)",
    page_icon="ğŸ›’",
    layout="wide"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets.get("SUPABASE_URL", "")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets.get("SUPABASE_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID") or st.secrets.get("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") or st.secrets.get("NAVER_CLIENT_SECRET", "")

# LangSmith ì„¤ì • (ì„ íƒì )
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY") or st.secrets.get("LANGSMITH_API_KEY", "")
if LANGSMITH_API_KEY:
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "smart-shopping-app"
    os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_API_KEY
else:
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

# CSS ìŠ¤íƒ€ì¼ (í—¤ë” ì• ë‹ˆë©”ì´ì…˜ ë° ìƒ‰ìƒ ë³€ê²½)
st.markdown("""
<style>
    /* í—¤ë” ê·¸ë¼ë°ì´ì…˜ ë°°ê²½ ì• ë‹ˆë©”ì´ì…˜ - í•˜ëŠ˜ìƒ‰ ê³„ì—´ë¡œ ë³€ê²½ */
    @keyframes gradientAnimation {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }

    /* ì œëª© ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
    @keyframes fadeInDown {
        from {
            opacity: 0;
            transform: translate3d(0, -20px, 0);
        }
        to {
            opacity: 1;
            transform: translate3d(0, 0, 0);
        }
    }

    /* ë¶€ì œëª©/ì¼ë°˜ í…ìŠ¤íŠ¸ í˜ì´ë“œì¸ ì• ë‹ˆë©”ì´ì…˜ */
    @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
    }

    /* í•˜ë‹¨ì—ì„œ ìœ„ë¡œ í˜ì´ë“œì¸ ì• ë‹ˆë©”ì´ì…˜ (ê²°ê³¼ í•­ëª© ë“±ì— ì‚¬ìš©) */
    @keyframes fadeInUp {
        from {
            opacity: 0;
            transform: translate3d(0, 20px, 0);
        }
        to {
            opacity: 1;
            transform: translate3d(0, 0, 0);
        }
    }

    /* ì•„ì´ì½˜ íšŒì „ ì• ë‹ˆë©”ì´ì…˜ */
    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }

    /* Main Header Styling */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        /* ê¸°ì¡´ ë³´ë¼ìƒ‰ ê³„ì—´ ëŒ€ì‹  ì˜ˆìœ í•˜ëŠ˜ìƒ‰ ê·¸ë¼ë°ì´ì…˜ìœ¼ë¡œ ë³€ê²½ */
        background: linear-gradient(90deg, #89cff0 0%, #4682b4 100%); /* ë°ì€ í•˜ëŠ˜ìƒ‰ -> ìŠ¤í‹¸ë¸”ë£¨ */
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        background-size: 200% 200%; /* ë°°ê²½ í¬ê¸°ë¥¼ í‚¤ì›Œ ì• ë‹ˆë©”ì´ì…˜ ì˜ì—­ í™•ë³´ */
        animation: gradientAnimation 10s ease infinite; /* 10ì´ˆ ë™ì•ˆ ë¶€ë“œëŸ½ê²Œ ë°˜ë³µ */
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* ê·¸ë¦¼ì ì¶”ê°€ë¡œ ì…ì²´ê° */
        overflow: hidden; /* ë‚´ë¶€ ìš”ì†Œ ë„˜ì¹¨ ë°©ì§€ */
    }
    .main-header h1 {
        font-size: 3.5rem;
        font-weight: bold;
        text-shadow: 3px 3px 5px rgba(0,0,0,0.3); /* ì œëª© ê·¸ë¦¼ì */
        animation: fadeInDown 1s ease-out; /* ì œëª© ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
        margin-bottom: 0.5rem;
    }
    .main-header p {
        font-size: 1.5rem;
        margin-top: 0;
        opacity: 0; /* ì´ˆê¸° íˆ¬ëª…í•˜ê²Œ ì„¤ì • */
        animation: fadeIn 1.5s ease-out 0.5s forwards; /* ë¶€ì œëª© í˜ì´ë“œì¸ (0.5ì´ˆ ì§€ì—°) */
    }

    /* Input and Button Styling */
    .stTextInput > div > div > input {
        border: 2px solid #89cff0; /* í•˜ëŠ˜ìƒ‰ ê³„ì—´ë¡œ ë³€ê²½ */
        border-radius: 8px;
        padding: 0.75rem 1rem;
        transition: all 0.3s ease;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    .stTextInput > div > div > input:focus {
        border-color: #4682b4 !important; /* í¬ì»¤ìŠ¤ ì‹œ ë” ì§„í•œ í•˜ëŠ˜ìƒ‰ */
        box-shadow: 0 0 0 0.25rem rgba(70, 130, 180, 0.25); /* í¬ì»¤ìŠ¤ ì‹œ ê·¸ë¦¼ìë„ ë³€ê²½ */
        outline: none;
    }

    .stButton > button {
        background-color: #4682b4; /* ë²„íŠ¼ ìƒ‰ìƒë„ ìŠ¤í‹¸ë¸”ë£¨ ê³„ì—´ë¡œ ë³€ê²½ */
        color: white;
        border-radius: 8px;
        padding: 0.75rem 1.5rem;
        font-size: 1.1rem;
        font-weight: bold;
        transition: all 0.3s ease;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
        border: none; /* Streamlit ê¸°ë³¸ í…Œë‘ë¦¬ ì œê±° */
    }
    .stButton > button:hover {
        background-color: #89cff0; /* í˜¸ë²„ ì‹œ ë°ì€ í•˜ëŠ˜ìƒ‰ */
        transform: translateY(-3px);
        box-shadow: 0 6px 15px rgba(0, 0, 0, 0.25);
    }
    /* ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œ ì•„ì´ì½˜ íšŒì „ ì• ë‹ˆë©”ì´ì…˜ í´ë˜ìŠ¤ */
    .stButton > button.search-button-clicked .st-emotion-cache-zt5ig8 { /* Streamlit ì•„ì´ì½˜ SVG ì„ íƒì */
        animation: rotate 0.5s ease-out;
    }

    /* Pros Section Styling */
    .pros-section {
        background-color: #e6ffe6; /* ê¸°ì¡´ ìœ ì§€ */
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 6px solid #28a745;
        box-shadow: 0 4px 12px rgba(40, 167, 69, 0.1);
        transition: all 0.3s ease-in-out;
    }
    .pros-section:hover {
        transform: translateY(-5px);
        box-shadow: 0 8px 20px rgba(40, 167, 69, 0.25);
    }
    .pros-section h3 {
        color: #28a745;
        font-size: 1.8rem;
        margin-bottom: 1rem;
    }
    .pros-section p { /* ê° í•­ëª©ì— ì ìš© */
        margin-bottom: 0.5rem;
        animation: fadeInUp 0.5s ease-out forwards; /* ê°œë³„ ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
        opacity: 0; /* ì´ˆê¸° íˆ¬ëª… */
    }
    /* ê° í•­ëª©ì— delayë¥¼ ì¤˜ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ í•¨ */
    .pros-section p:nth-child(2) { animation-delay: 0.1s; }
    .pros-section p:nth-child(3) { animation-delay: 0.2s; }
    .pros-section p:nth-child(4) { animation-delay: 0.3s; }
    .pros-section p:nth-child(5) { animation-delay: 0.4s; }
    .pros-section p:nth-child(6) { animation-delay: 0.5s; }
    /* ... í•„ìš”í•œ ë§Œí¼ ì¶”ê°€ (ìµœëŒ€ 10ê°œ) */


    /* Cons Section Styling */
    .cons-section {
        background-color: #ffe6e6; /* ê¸°ì¡´ ìœ ì§€ */
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 6px solid #dc3545;
        box-shadow: 0 4px 12px rgba(220, 53, 69, 0.1);
        transition: all 0.3s ease-in-out;
    }
    .cons-section:hover {
        transform: translateY(-5px);
        box-shadow: 0 8px 20px rgba(220, 53, 69, 0.25);
    }
    .cons-section h3 {
        color: #dc3545;
        font-size: 1.8rem;
        margin-bottom: 1rem;
    }
    .cons-section p { /* ê° í•­ëª©ì— ì ìš© */
        margin-bottom: 0.5rem;
        animation: fadeInUp 0.5s ease-out forwards; /* ê°œë³„ ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
        opacity: 0; /* ì´ˆê¸° íˆ¬ëª… */
    }
    /* ê° í•­ëª©ì— delayë¥¼ ì¤˜ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ í•¨ */
    .cons-section p:nth-child(2) { animation-delay: 0.1s; }
    .cons-section p:nth-child(3) { animation-delay: 0.2s; }
    .cons-section p:nth-child(4) { animation-delay: 0.3s; }
    .cons-section p:nth-child(5) { animation-delay: 0.4s; }
    .cons-section p:nth-child(6) { animation-delay: 0.5s; }
    /* ... í•„ìš”í•œ ë§Œí¼ ì¶”ê°€ */

    /* Process Info Styling */
    .process-info {
        background-color: #e0f2fe; /* ê¸°ì¡´ ìœ ì§€ */
        padding: 1.2rem;
        border-radius: 8px;
        margin: 1.5rem 0;
        border-left: 4px solid #2196f3;
        box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1);
        animation: fadeInUp 0.8s ease-out; /* ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
    }

    /* Footer Styling */
    .footer {
        text-align: center;
        color: #888;
        padding: 2rem;
        font-size: 0.9rem;
    }
    .stExpander { /* Expanderì— ê·¸ë¦¼ì ì¶”ê°€ */
        box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        border-radius: 8px;
        margin-top: 1rem;
    }

</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ›’ ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ <br> (LangGraph Edition)</h1>
    <p>
        LangGraphë¡œ êµ¬í˜„í•œ ì§€ëŠ¥í˜• ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ
    </p>
</div>
""", unsafe_allow_html=True)

# ========================
# LangGraph State ì •ì˜
# ========================

class SearchState(TypedDict):
    """ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ"""
    product_name: str
    search_method: str  # "database" or "web_crawling"
    results: dict
    pros: List[str]
    cons: List[str]
    sources: List[dict]
    messages: Annotated[List[Union[HumanMessage, AIMessage]], operator.add]
    error: str

# ========================
# í¬ë¡¤ë§ í´ë˜ìŠ¤
# ========================

class ProConsLaptopCrawler:
    def __init__(self, naver_client_id, naver_client_secret):
        self.naver_headers = {
            "X-Naver-Client-Id": naver_client_id,
            "X-Naver-Client-Secret": naver_client_secret
        }
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
        # í†µê³„
        self.stats = {
            'total_crawled': 0,
            'valid_pros_cons': 0,
            'api_errors': 0
        }
    
    def remove_html_tags(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        text = BeautifulSoup(text, "html.parser").get_text()
        text = re.sub(r'<[^>]+>', '', text)
        return text.strip()
    
    def search_blog(self, query, display=20):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
        url = "https://openapi.naver.com/v1/search/blog"
        params = {
            "query": query,
            "display": display,
            "sort": "sim"
        }
        
        try:
            response = requests.get(url, headers=self.naver_headers, params=params)
            if response.status_code == 200:
                result = response.json()
                for item in result.get('items', []):
                    item['title'] = self.remove_html_tags(item['title'])
                    item['description'] = self.remove_html_tags(item['description'])
                return result
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None
    
    def crawl_content(self, url):
        """ë¸”ë¡œê·¸ ë³¸ë¬¸ í¬ë¡¤ë§"""
        try:
            if "blog.naver.com" in url:
                parts = url.split('/')
                if len(parts) >= 5:
                    blog_id = parts[3]
                    post_no = parts[4].split('?')[0]
                    mobile_url = f"https://m.blog.naver.com/{blog_id}/{post_no}"
                    
                    response = requests.get(mobile_url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        content = ""
                        for selector in ['div.se-main-container', 'div#postViewArea', 'div.post_ct']:
                            elem = soup.select_one(selector)
                            if elem:
                                content = elem.get_text(separator='\n', strip=True)
                                break
                        
                        if not content:
                            content = soup.get_text(separator='\n', strip=True)
                        
                        content = re.sub(r'\s+', ' ', content)
                        content = content.replace('\u200b', '')
                        
                        return content if len(content) > 300 else None
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return None
    
    def extract_pros_cons_with_gpt(self, product_name, content):
        """ChatGPTë¡œ ì¥ë‹¨ì  ì¶”ì¶œ"""
        if not content or len(content) < 200:
            return None
        
        content_preview = content[:1500]
        
        prompt = f"""ë‹¤ìŒì€ "{product_name}"ì— ëŒ€í•œ ë¸”ë¡œê·¸ ë¦¬ë·°ì…ë‹ˆë‹¤.

[ë¸”ë¡œê·¸ ë‚´ìš©]
{content_preview}

ìœ„ ë‚´ìš©ì—ì„œ {product_name}ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ì‚¬ìš© ê²½í—˜ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì¥ì :
- (êµ¬ì²´ì ì¸ ì¥ì  1)
- (êµ¬ì²´ì ì¸ ì¥ì  2)
- (êµ¬ì²´ì ì¸ ì¥ì  3)

ë‹¨ì :
- (êµ¬ì²´ì ì¸ ë‹¨ì  1)
- (êµ¬ì²´ì ì¸ ë‹¨ì  2)
- (êµ¬ì²´ì ì¸ ë‹¨ì  3)

ë§Œì•½ ì¥ë‹¨ì  ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ "ì •ë³´ ë¶€ì¡±"ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."""
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš© ê²½í—˜ì— ê¸°ë°˜í•œ ì¥ë‹¨ì ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            result = response.choices[0].message.content.strip()
            
            if result and "ì •ë³´ ë¶€ì¡±" not in result:
                pros = []
                cons = []
                
                lines = result.split('\n')
                current_section = None
                
                for line in lines:
                    line = line.strip()
                    if 'ì¥ì :' in line or 'ì¥ì  :' in line:
                        current_section = 'pros'
                    elif 'ë‹¨ì :' in line or 'ë‹¨ì  :' in line:
                        current_section = 'cons'
                    elif line.startswith('-') and current_section:
                        point = line[1:].strip()
                        if point and len(point) > 5:
                            if current_section == 'pros':
                                pros.append(point)
                            else:
                                cons.append(point)
                
                if pros or cons:
                    self.stats['valid_pros_cons'] += 1
                    return {
                        'pros': pros[:5],
                        'cons': cons[:5]
                    }
            
            return None
                
        except Exception as e:
            self.stats['api_errors'] += 1
            print(f"GPT API ì˜¤ë¥˜: {str(e)[:100]}")
            return None
    
    def deduplicate_points(self, points):
        """ìœ ì‚¬í•œ ì¥ë‹¨ì  ì¤‘ë³µ ì œê±°"""
        if not points:
            return []
        
        unique_points = []
        seen_keywords = set()
        
        for point in points:
            keywords = set(word for word in point.split() if len(word) > 2)
            
            if len(keywords & seen_keywords) < len(keywords) * 0.5:
                unique_points.append(point)
                seen_keywords.update(keywords)
            
            if len(unique_points) >= 10:
                break
        
        return unique_points

# ========================
# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
# ========================

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_supabase_client():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

@st.cache_resource
def get_crawler():
    return ProConsLaptopCrawler(NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)

def search_database(state: SearchState) -> SearchState:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œí’ˆ ê²€ìƒ‰"""
    product_name = state["product_name"]
    supabase = get_supabase_client()
    
    state["messages"].append(
        HumanMessage(content=f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ê²€ìƒ‰ ì¤‘...")
    )
    
    try:
        # ì •í™•í•œ ë§¤ì¹­ë§Œ ì‹œë„ (ë²¡í„° ê²€ìƒ‰ ì œê±°)
        exact_match = supabase.table('laptop_pros_cons').select("*").eq('product_name', product_name).execute()
        if exact_match.data:
            state["search_method"] = "database"
            state["results"] = {"data": exact_match.data}
            state["messages"].append(
                AIMessage(content=f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! ({len(exact_match.data)}ê°œ í•­ëª©)")
            )
            return state
        
        state["messages"].append(
            AIMessage(content=f"âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        )
        state["results"] = {"data": None}
        return state
        
    except Exception as e:
        state["error"] = str(e)
        state["messages"].append(
            AIMessage(content=f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["results"] = {"data": None}
        return state

def crawl_web(state: SearchState) -> SearchState:
    """ì›¹ì—ì„œ ì œí’ˆ ì •ë³´ í¬ë¡¤ë§"""
    if state["results"].get("data"):  # ì´ë¯¸ DBì—ì„œ ì°¾ì€ ê²½ìš°
        return state
    
    product_name = state["product_name"]
    state["search_method"] = "web_crawling"
    crawler = get_crawler()
    
    state["messages"].append(
        HumanMessage(content=f"ğŸŒ ì›¹ì—ì„œ '{product_name}' ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...")
    )
    
    all_pros = []
    all_cons = []
    sources = []
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ - ì œí’ˆëª…ì— ë§ì¶° ë™ì ìœ¼ë¡œ ìƒì„±
    search_queries = [
        f"{product_name} ì¥ë‹¨ì  ì‹¤ì‚¬ìš©",
        f"{product_name} ë‹¨ì  í›„ê¸°",
        f"{product_name} ì¥ì  ë¦¬ë·°"
    ]
    
    for query in search_queries:
        state["messages"].append(
            AIMessage(content=f"ğŸ” ê²€ìƒ‰ì–´: '{query}'")
        )
        
        # ë„¤ì´ë²„ ê²€ìƒ‰
        result = crawler.search_blog(query, display=10)
        if not result or 'items' not in result:
            continue
        
        posts = result['items']
        state["messages"].append(
            AIMessage(content=f"â†’ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ë°œê²¬")
        )
        
        # ê° í¬ìŠ¤íŠ¸ ì²˜ë¦¬
        for idx, post in enumerate(posts[:5]):  # ê° ì¿¼ë¦¬ë‹¹ ìµœëŒ€ 5ê°œ
            state["messages"].append(
                AIMessage(content=f"ğŸ“– ë¶„ì„ ì¤‘: {post['title'][:40]}...")
            )
            
            # í¬ë¡¤ë§
            content = crawler.crawl_content(post['link'])
            if not content:
                continue
            
            crawler.stats['total_crawled'] += 1
            
            # ì¥ë‹¨ì  ì¶”ì¶œ
            pros_cons = crawler.extract_pros_cons_with_gpt(product_name, content)
            
            if pros_cons:
                all_pros.extend(pros_cons['pros'])
                all_cons.extend(pros_cons['cons'])
                sources.append({
                    'title': post['title'],
                    'link': post['link'],
                    'date': post.get('postdate', '')
                })
                
                state["messages"].append(
                    AIMessage(content=f"âœ“ ì¥ì  {len(pros_cons['pros'])}ê°œ, ë‹¨ì  {len(pros_cons['cons'])}ê°œ ì¶”ì¶œ")
                )
            
            time.sleep(1)  # API ì œí•œ ë°©ì§€
        
        time.sleep(2)  # ê²€ìƒ‰ ê°„ ëŒ€ê¸°
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_pros = crawler.deduplicate_points(all_pros)
    unique_cons = crawler.deduplicate_points(all_cons)
    
    state["pros"] = unique_pros
    state["cons"] = unique_cons
    state["sources"] = sources[:10]
    
    if state["pros"] or state["cons"]:
        state["messages"].append(
            AIMessage(content=f"ğŸ‰ ì›¹ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ ìˆ˜ì§‘")
        )
        
        # DBì— ì €ì¥
        try:
            supabase = get_supabase_client()
            data = []
            
            for pro in state["pros"]:
                data.append({
                    'product_name': product_name,
                    'type': 'pro',
                    'content': pro
                })
            
            for con in state["cons"]:
                data.append({
                    'product_name': product_name,
                    'type': 'con',
                    'content': con
                })
            
            if data:
                supabase.table('laptop_pros_cons').insert(data).execute()
                state["messages"].append(
                    AIMessage(content="ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ!")
                )
        except Exception as e:
            state["messages"].append(
                AIMessage(content=f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            )
    else:
        # st.error(f"'{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") # ì´ ë¶€ë¶„ì„ UIì— ì§ì ‘ í‘œì‹œí•˜ë„ë¡ ë³€ê²½
        state["messages"].append(
            AIMessage(content=f"ğŸ˜¢ '{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        )
    
    # ìµœì¢… í†µê³„
    state["messages"].append(
        AIMessage(content=f"ğŸ“Š í¬ë¡¤ë§ í†µê³„: ì´ {crawler.stats['total_crawled']}ê°œ í˜ì´ì§€, ìœ íš¨ ì¶”ì¶œ {crawler.stats['valid_pros_cons']}ê°œ")
    )
    
    return state

def process_results(state: SearchState) -> SearchState:
    """ê²°ê³¼ ì²˜ë¦¬ ë° ì •ë¦¬"""
    if state["search_method"] == "database" and state["results"].get("data"):
        # DB ê²°ê³¼ ì²˜ë¦¬
        data = state["results"]["data"]
        state["pros"] = [item['content'] for item in data if item['type'] == 'pro']
        state["cons"] = [item['content'] for item in data if item['type'] == 'con']
        state["sources"] = []  # DBì—ëŠ” ë³„ë„ ì†ŒìŠ¤ ì—†ìŒ
        
        state["messages"].append(
            AIMessage(content=f"ğŸ“‹ ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ: ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ")
        )
    
    return state

def should_search_web(state: SearchState) -> str:
    """ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨"""
    if state["results"].get("data"):
        return "process"
    else:
        return "crawl"

# ========================
# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
# ========================

@st.cache_resource
def create_search_workflow():
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_db", search_database)
    workflow.add_node("crawl_web", crawl_web)
    workflow.add_node("process", process_results)
    
    # ì—£ì§€ ì„¤ì •
    workflow.set_entry_point("search_db")
    workflow.add_conditional_edges(
        "search_db",
        should_search_web,
        {
            "crawl": "crawl_web",
            "process": "process"
        }
    )
    workflow.add_edge("crawl_web", "process")
    workflow.add_edge("process", END)
    
    return workflow.compile()

# ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
search_app = create_search_workflow()

# ========================
# Streamlit UI
# ========================

# ê²€ìƒ‰ ì„¹ì…˜
col1, col2, col3 = st.columns([1, 3, 1])

with col2:
    product_name = st.text_input(
        "ğŸ” ì œí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: ë§¥ë¶ í”„ë¡œ M3, LG ê·¸ë¨ 2024, ê°¤ëŸ­ì‹œë¶4 í”„ë¡œ, ê·¸ë¦´ ìš”ê±°íŠ¸",
        key="product_input" # ê³ ìœ  í‚¤ ì¶”ê°€
    )
    
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        # ë²„íŠ¼ì— í´ë¦­ ì‹œ í´ë˜ìŠ¤ ì¶”ê°€ë¥¼ ìœ„í•œ íŠ¸ë¦­ (ìë°”ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…)
        search_button_clicked = st.button("ğŸ” ê²€ìƒ‰í•˜ê¸°", use_container_width=True, type="primary", key="search_button")
        if search_button_clicked:
            st.markdown(
                """
                <script>
                    const button = window.parent.document.querySelector('[data-testid="stButton"] button');
                    if (button) {
                        button.classList.add('search-button-clicked');
                        setTimeout(() => {
                            button.classList.remove('search-button-clicked');
                        }, 500); // 0.5ì´ˆ í›„ í´ë˜ìŠ¤ ì œê±°
                    }
                </script>
                """,
                unsafe_allow_html=True
            )
            # ë²„íŠ¼ í´ë¦­ ì‹œ ì…ë ¥ í•„ë“œ í¬ì»¤ìŠ¤ í•´ì œ (í‚¤ë³´ë“œ ë‹«í˜)
            # st.session_state["product_input"] = product_name # ì…ë ¥ê°’ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ì´ê²ƒì€ ì´ì œ ë¶ˆí•„ìš”)
            # st.experimental_rerun() # ì¬ì‹¤í–‰í•˜ì—¬ CSS ë³€ê²½ ì ìš© (ì´ê²ƒì€ ë²„íŠ¼ í´ë¦­ í›„ ë°”ë¡œ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±°)
    with col_btn2:
        show_process = st.checkbox("ğŸ”§ í”„ë¡œì„¸ìŠ¤ ë³´ê¸°", value=True)

# ê²€ìƒ‰ ì‹¤í–‰
if search_button_clicked and product_name:
    with st.spinner(f"'{product_name}' ê²€ìƒ‰ ì¤‘..."):
        # LangGraph ì‹¤í–‰
        initial_state = {
            "product_name": product_name,
            "search_method": "",
            "results": {},
            "pros": [],
            "cons": [],
            "sources": [],
            "messages": [],
            "error": ""
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        final_state = search_app.invoke(initial_state)
    
    # í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ í‘œì‹œ
    if show_process and final_state["messages"]:
        with st.expander("ğŸ”§ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤", expanded=True):
            for msg in final_state["messages"]:
                if isinstance(msg, HumanMessage):
                    st.info(f"ğŸ‘¤ {msg.content}") # infoë¡œ ë³€ê²½í•˜ì—¬ ì‹œê°ì  êµ¬ë¶„
                else:
                    st.success(f"ğŸ¤– {msg.content}") # successë¡œ ë³€ê²½
    
    # ê²°ê³¼ í‘œì‹œ
    if final_state["pros"] or final_state["cons"]:
        # ê²€ìƒ‰ ì •ë³´
        st.markdown(f"""
        <div class="process-info">
            <strong>ê²€ìƒ‰ ë°©ë²•:</strong> {
                'ë°ì´í„°ë² ì´ìŠ¤' if final_state["search_method"] == "database" else 'ì›¹ í¬ë¡¤ë§'
            } | 
            <strong>ì¥ì :</strong> {len(final_state["pros"])}ê°œ | 
            <strong>ë‹¨ì :</strong> {len(final_state["cons"])}ê°œ
        </div>
        """, unsafe_allow_html=True)
        
        # ì¥ë‹¨ì  í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="pros-section">
                <h3>âœ… ì¥ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            if final_state["pros"]:
                # ê° ì¥ì  í•­ëª©ì— ê³ ìœ í•œ ì§€ì—° ì‹œê°„ ë¶€ì—¬
                for idx, pro in enumerate(final_state["pros"], 1):
                    # st.markdownì„ ì‚¬ìš©í•˜ì—¬ ê° í•­ëª©ì— CSS ì• ë‹ˆë©”ì´ì…˜ í´ë˜ìŠ¤ ì ìš©
                    st.markdown(f'<p style="animation-delay: {idx*0.1}s;">ğŸŸ¢ {idx}. {pro}</p>', unsafe_allow_html=True)
            else:
                st.write("ì¥ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            st.markdown("""
            <div class="cons-section">
                <h3>âŒ ë‹¨ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            if final_state["cons"]:
                # ê° ë‹¨ì  í•­ëª©ì— ê³ ìœ í•œ ì§€ì—° ì‹œê°„ ë¶€ì—¬
                for idx, con in enumerate(final_state["cons"], 1):
                    st.markdown(f'<p style="animation-delay: {idx*0.1}s;">ğŸ”´ {idx}. {con}</p>', unsafe_allow_html=True)
            else:
                st.write("ë‹¨ì  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¶œì²˜ (ì›¹ í¬ë¡¤ë§ì¸ ê²½ìš°)
        if final_state["sources"]:
            with st.expander("ğŸ“š ì¶œì²˜ ë³´ê¸°"):
                for idx, source in enumerate(final_state["sources"], 1):
                    st.write(f"{idx}. [{source['title']}]({source['link']})")
        
        # í†µê³„
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ì¥ì ", f"{len(final_state['pros'])}ê°œ")
        with col2:
            st.metric("ì´ ë‹¨ì ", f"{len(final_state['cons'])}ê°œ")
        with col3:
            st.metric("ê²€ìƒ‰ ë°©ë²•", "DB" if final_state["search_method"] == "database" else "ì›¹")
    else:
        st.error(f"'{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    st.info("ğŸ’¡ LangGraphë¡œ êµ¬í˜„ëœ ì²´ê³„ì ì¸ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤")
with col2:
    st.info("ğŸ”„ DB ìš°ì„  ê²€ìƒ‰ â†’ ì—†ìœ¼ë©´ ì›¹ í¬ë¡¤ë§")
with col3:
    st.info("ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ ìë™ ì €ì¥")

current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
st.markdown(f"""
<div class="footer">
    <p>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {current_date}</p>
    <p>Powered by LangGraph & OpenAI</p>
</div>
""", unsafe_allow_html=True)
